{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=BATCH_SIZE):\n",
    "    # Training normalization and augmentation\n",
    "    train_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "    ])\n",
    "\n",
    "    # Test normalization (but no augmentation)\n",
    "    test_transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=0.1307, std=0.3081)\n",
    "    ])\n",
    "\n",
    "    # Load the dataset and apply the transformations\n",
    "    train_data = torchvision.datasets.MNIST('./datafiles/', train=True, download=True, transform=train_transform)\n",
    "    test_data = torchvision.datasets.MNIST('./datafiles/', train=False, download=True, transform=test_transform)\n",
    "\n",
    "    # Note: Iterating through the dataloader yields batches of (inputs, targets)\n",
    "    # Where inputs is a torch.Tensor of shape (B, 1, 28, 28) and targets is a torch.Tensor of shape (B,)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000)\n",
    "\n",
    "    return train_data, test_data, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformations(train_data):\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(5, 6))\n",
    "\n",
    "    plot_images = []\n",
    "    plot_labels = []\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten(), start=1000):\n",
    "        (image, label) = train_data[i]\n",
    "\n",
    "        # Save this data for later\n",
    "        plot_images.append(image)\n",
    "        plot_labels.append(label)\n",
    "\n",
    "        # Plot each image\n",
    "        ax.imshow(image.squeeze(), cmap=\"gray\")\n",
    "        ax.set_title(f\"Label: {label}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyVGG, self).__init__()\n",
    "\n",
    "        # Make the image smaller but deeper\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 1x28x28 --> 2x14x14\n",
    "            nn.Conv2d(1, 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2: 2x14x14 --> 4x7x7\n",
    "            nn.Conv2d(2, 4, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3: 4x7x7 --> 8x3x3\n",
    "            nn.Conv2d(4, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Make predictions based on the fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Block 4: 8x3x3 --> 30 --> 10 classes\n",
    "            nn.Linear(8 * 3 * 3, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(30, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)        # Feature extractor\n",
    "        x = x.view(x.size(0), -1)   # Flatten the features into [batch_size, channels * height * width]\n",
    "        x = self.classifier(x)      # Classifier\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, epoch=-1):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track some values to compute statistics\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=-1)\n",
    "        all_predictions.extend(preds.detach().cpu().tolist())\n",
    "        all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1} --> Train loss = {final_loss:.2f}, Train accuracy = {acc * 100:.3f}%\")\n",
    "    return acc, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, loss_fn, epoch=-1):\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs.to(DEVICE))\n",
    "            loss = loss_fn(outputs, targets.to(DEVICE))\n",
    "\n",
    "            # Track some values to compute statistics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            all_predictions.extend(preds.detach().cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    final_loss = total_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch + 1} --> Test loss = {final_loss:.2f}, Test accuracy = {acc * 100:.3f}%\")\n",
    "    return acc, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_loader, test_loader = load_data()\n",
    "plot_transformations(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = MyVGG()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-8)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_metrics = []\n",
    "test_metrics = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss = train(model, train_loader, loss_fn, optimizer, epoch)\n",
    "    test_acc, test_loss = test(model, test_loader, loss_fn, epoch)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_metrics.append(train_acc)\n",
    "    test_metrics.append(test_acc)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axs[0].plot(train_losses, c=\"r\", label=\"Train loss\")\n",
    "axs[0].plot(test_losses, c=\"b\", label=\"Test loss\")\n",
    "axs[0].legend()\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "axs[1].plot(train_metrics, \"o-\", c=\"r\", label=\"Train accuracy\")\n",
    "axs[1].plot(test_metrics, \"o-\", c=\"b\", label=\"Test accuracy\")\n",
    "axs[1].legend()\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde599",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
